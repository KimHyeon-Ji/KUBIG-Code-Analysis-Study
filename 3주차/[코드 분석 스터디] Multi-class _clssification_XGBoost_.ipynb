{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"캐글_코드_분석_스터디_Multi-class _clssification_XGBoost_.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"jE3pbpZinBb6"},"source":["# 간략한 대회 소개\n","\n","kaggle 대회 링크 : https://www.kaggle.com/c/costa-rican-household-poverty-prediction\n","\n","## 대회의 목적\n"," 미국을 비롯한 여러 나라의 은행들과 기관들은 도움이 필요한 사람들에게 금전적인 지원을 하고 있다. 하지만 이러한 단체들은 수많은 사람들 중 과연 누가 이러한 지원이 필요한 사람인지를 정확히 판별함에 있어서 어려움을 겪고 있다. 이 대회는 이러한 어려움을 해소하기 위해서 다양한 변수들을 제공하고 이를 통해 도움이 필요한 사람들을 분류하는 모델링을 진행하는 것이 이 대회의 목적이다\n","\n","## 데이터 셋, 커널의 특징\n"," 142개의 column 23856개 value가 있는 데이터셋으로 categorical 형의 변수가 많다. 또한 이 커널에서는 굉장히 많은 변수들을 새로 만들어 모델링에 사용한다. 다양한 categorical 변수들에 대해서 변환을 진행하는 것을 확인할 수 있다.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9th_LwDd0toW"},"source":["## XGBoost에 대한 간단한 설명\n","\n","참고 블로그 : https://bcho.tistory.com/1354\n","\n","boosting 기법으로 여러 개의 약한 Decision Tree들을 조합하여 사용하는 ensemble 기법 중 하나이다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LYQ0H_qBMect"},"source":["# Voting classifier에 관한 설명\n","\n","참고 블로그 : https://nonmeyet.tistory.com/entry/Python-Voting-Classifiers%EB%8B%A4%EC%88%98%EA%B2%B0-%EB%B6%84%EB%A5%98%EC%9D%98-%EC%A0%95%EC%9D%98%EC%99%80-%EA%B5%AC%ED%98%84\n","\n","여러 모델들 중 가장 뛰어난 성능을 보이는 모델을 선택\n","\n","hard 와 soft로 나누어짐"]},{"cell_type":"markdown","metadata":{"id":"GI0AWccs6bfL"},"source":["# 주요 변수 설명\n","\n","Id - a unique identifier for each row.\n","\n","Target - the target is an ordinal variable indicating groups of income levels. \n","\n","1 = extreme poverty \n","\n","2 = moderate poverty \n","\n","3 = vulnerable households \n","\n","4 = non vulnerable households\n","\n","idhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.\n","\n","parentesco1 - indicates if this person is the head of the household.\n","\n","This data contains 142 total columns."]},{"cell_type":"code","metadata":{"id":"vXQl4_DAzdAp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7hLHHzDr0el8"},"source":["# LGMB with random split for early stopping\n","\n","\n","\n","*   The LightGBM models have been replaced with XGBoost and the code has been updated accordingly\n","*   Fitting VotingClassifiers of RandomForests and ensembling the results of the XGBs with the RFs\n","*   Some additional features have been added\n","*   Some features which were previously dropped have been retained\n","*   Some of the code has been reorganized\n","*   Rather than splitting the data once and using the validation data for the LGBM early stopping, I split the data during the training so the entire training set can be trained on. I found that this works better than k-fold split in this case\n","\n","\n","\n","## Several key points\n","\n","*   This kernel runs training on the heads of housholds only.\n","*   It seems to be very importtant to balance class frequencies\n","*   This kernel uses macor F1 score to early stopping in training\n","*   Categoricals are turned into numbers with proper mapping instead of blind label encoding\n","*   OHE if reveresed into label encoding, as it is easier to eigest for a tree model\n","*   idhogar is NOT used in training\n","*   There are aggregations done within households and new features are hand-crafted\n","*   A voting classifier is used to average over several LightGBM models\n"]},{"cell_type":"markdown","metadata":{"_uuid":"0479e25c001874fda15f875d76f8663885caa138","id":"iOkxkiRl_6og"},"source":["# LGMB with random split for early stopping\n","\n","**Edits by Eric Antoine Scuccimarra** - This is a fork of  https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro, by Misha Losvyi, with a few changes:\n"," - The LightGBM models have been replaced with XGBoost and the code has been updated accordingly.\n"," - I am also fitting VotingClassifiers of RandomForests and ensembling the results of the XGBs with the RFs.\n"," - Some additional features have been added.\n"," - Some features which were previously dropped have been retained.\n"," - Some of the code has been reorganized.\n"," - Rather than splitting the data once and using the validation data for the LGBM early stopping, I split the data during the training so the entire training set can be trained on. I found that this works better than a k-fold split in this case.\n","\n","Some additional features were taken from: https://www.kaggle.com/kuriyaman1002/reduce-features-140-84-keeping-f1-score, by Kuriyaman.\n"," \n","**Notes from Original Kernel (edited by EAS):**\n","\n","This kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster. \n","\n","Several key points:\n","- **This kernel runs training on the heads of housholds only** (after extracting aggregates over households). This follows the announced scoring startegy: *Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored.* (from the data description). However, at the moment it seems that evaluation depends also on non-head household members, see https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360115. In practise, ful prediction gives ~0.4 PLB score, while replacing all non-head entries with class 1 leads to a drop down to ~0.2 PLB score\n","- **It seems to be very important to balance class frequencies.** Without balancing a trained model gives ~0.39 PLB / ~0.43 local test, while adding balancing leads to ~0.42 PLB / 0.47 local test. One can do it by hand, one can achieve it by undersampling. But the simplest (and more powerful compared to undersampling) is to set `class_weight='balanced'` in the LightGBM model constructor in sklearn API.\n","- **This kernel uses macro F1 score to early stopping in training**. This is done to align with the scoring strategy.\n","- Categoricals are turned into numbers with proper mapping instead of blind label encoding. \n","- **OHE if reversed into label encoding, as it is easier to digest for a tree model.** This trick would be harmful for non-tree models, so be careful.\n","- **idhogar is NOT used in training**. The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :)\n","- **There are aggregations done within households and new features are hand-crafted**. Note, that there are not so many features that can be aggregated, as most are already quoted on household level.\n","- **A voting classifier is used to average over several LightGBM models**"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-09-21T06:10:35.292318Z","iopub.execute_input":"2021-09-21T06:10:35.292617Z","iopub.status.idle":"2021-09-21T06:10:35.297736Z","shell.execute_reply.started":"2021-09-21T06:10:35.292573Z","shell.execute_reply":"2021-09-21T06:10:35.296657Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"MzimRzIJ_6oj","executionInfo":{"status":"ok","timestamp":1632293370869,"user_tz":-540,"elapsed":21381,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"64303e16-2da5-4ff6-ee94-7df6f6799899"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"8vQkdu43_6ok"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-09-21T06:09:58.600538Z","iopub.execute_input":"2021-09-21T06:09:58.600805Z","iopub.status.idle":"2021-09-21T06:09:58.717784Z","shell.execute_reply.started":"2021-09-21T06:09:58.600762Z","shell.execute_reply":"2021-09-21T06:09:58.716666Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"uEUrWNX__6ok","executionInfo":{"status":"ok","timestamp":1632293374519,"user_tz":-540,"elapsed":1578,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"a764894d-78b6-49cd-df40-c5d13deaabf0"},"source":["import numpy as np # linear algebra\n","import pandas as pd \n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline \n","\n","import lightgbm as lgb\n","import xgboost as xgb\n","from sklearn.metrics import f1_score\n","from sklearn.externals.joblib import Parallel, delayed\n","from sklearn.base import clone\n","from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n","from sklearn.utils import class_weight\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","metadata":{"_uuid":"8e4e08a17549fd247619178c96c3ade2519e9773","id":"m_x3nkba_6ol"},"source":["The following categorical mapping originates from [this kernel](https://www.kaggle.com/mlisovyi/categorical-variables-encoding-function)."]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"9UmzjtMZ_6ol"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# this only transforms the idhogar field, the other things this function used to do are done elsewhere\n","def encode_data(df):\n","    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n","\n","# plot feature importance for sklearn decision trees    \n","def feature_importance(forest, X_train, display_results=True):\n","    ranked_list = []\n","    zero_features = []\n","    \n","    importances = forest.feature_importances_\n","\n","    indices = np.argsort(importances)[::-1]\n","    \n","    if display_results:\n","        # Print the feature ranking\n","        print(\"Feature ranking:\")\n","\n","    for f in range(X_train.shape[1]):\n","        if display_results:\n","            print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]) + \" - \" + X_train.columns[indices[f]])\n","        \n","        ranked_list.append(X_train.columns[indices[f]])\n","        \n","        if importances[indices[f]] == 0.0:\n","            zero_features.append(X_train.columns[indices[f]])\n","            \n","    return ranked_list, zero_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"1785c8ca3467a7e95d007a2c5f36e39919fc0910","id":"XDeYhlEk_6om"},"source":["**There is also feature engineering magic happening here:**"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_uuid":"9c9f13e54fc2af9f938b895959631e1aeb3b08a2","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"pdU7u9ud_6om"},"source":["def do_features(df):\n","    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n","                 ('working_man_fraction', 'r4h2', 'r4t3'),\n","                 ('all_man_fraction', 'r4h3', 'r4t3'),\n","                 ('human_density', 'tamviv', 'rooms'),\n","                 ('human_bed_density', 'tamviv', 'bedrooms'),\n","                 ('rent_per_person', 'v2a1', 'r4t3'),\n","                 ('rent_per_room', 'v2a1', 'rooms'),\n","                 ('mobile_density', 'qmobilephone', 'r4t3'),\n","                 ('tablet_density', 'v18q1', 'r4t3'),\n","                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n","                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n","                ]\n","    \n","    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n","                 ('people_weird_stat', 'tamhog', 'r4t3')]\n","\n","    for f_new, f1, f2 in feats_div:\n","        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n","    for f_new, f1, f2 in feats_sub:\n","        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n","    \n","    # aggregation rules over household\n","    #aggregation rules over household 가정을 집합시키는 기준\n","    aggs_num = {'age': ['min', 'max', 'mean'],\n","                'escolari': ['min', 'max', 'mean']\n","               }\n","    \n","    aggs_cat = {'dis': ['mean']}\n","    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n","        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n","            aggs_cat[f_] = ['mean', 'count']\n","\n","    # aggregation over household\n","    for name_, df_ in [('18', df.query('age >= 18'))]:\n","        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n","        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n","        df = df.join(df_agg, how='left', on='idhogar')\n","        del df_agg\n","\n","    # Drop id's\n","    df.drop(['Id'], axis=1, inplace=True)\n","    \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_uuid":"a7df32a07c9157ee02ff9688cdc70c69e7571aae","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"f3u0Ok4r_6on"},"source":["# convert one hot encoded fields to label encoding\n","def convert_OHE2LE(df):\n","    tmp_df = df.copy(deep=True)\n","    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n","               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n","               'instlevel', 'lugar', 'tipovivi',\n","               'manual_elec']:\n","        if 'manual_' not in s_:\n","            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n","        elif 'elec' in s_:\n","            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n","        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n","        #deal with those OHE, where there is a sum over columns == 0\n","        if 0 in sum_ohe:\n","            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n","                  .format(s_))\n","            # dummy colmn name to be added\n","            col_dummy = s_+'_dummy'\n","            # add the column to the dataframe\n","            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n","            # add the name to the list of columns to be label-encoded\n","            cols_s_.append(col_dummy)\n","            # proof-check, that now the category is complete\n","            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n","            if 0 in sum_ohe:\n","                 print(\"The category completion did not work\")\n","        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)#최대 값을 가지는 인덱스 레이블을 따로 꺼내 저장한다\n","        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n","        if 'parentesco1' in cols_s_:\n","            cols_s_.remove('parentesco1')\n","        tmp_df.drop(cols_s_, axis=1, inplace=True)\n","    return tmp_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"eab84429fc9893c82e33b8319161c190b4104e9f","id":"INYrqMiT_6on"},"source":["# Read in the data and clean it up"]},{"cell_type":"code","metadata":{"_uuid":"e6f696a1677230c565532f141a02852e7c69b2e1","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"7WtGOfmM_6on"},"source":["train = pd.read_csv('/content/drive/MyDrive/train.csv')\n","test = pd.read_csv('/content/drive/MyDrive/test.csv')\n","\n","test_ids = test.Id"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"bd1f66cbdbfa4741d19a8b1f53793b967d62d281","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"T4RTDQyg_6oo"},"source":["def process_df(df_):\n","    # encode the idhogar\n","    encode_data(df_)\n","    \n","    # create aggregate features\n","    return do_features(df_)\n","\n","train = process_df(train)\n","test = process_df(test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"aaf8a52a939c7d5bc548cbe4ecc1458caae60e7d","id":"8cWUaBzp_6oo"},"source":["Clean up some missing data and convert objects to numeric."]},{"cell_type":"code","metadata":{"_uuid":"65dab0e9a94e8f87a7b73e7ec2c6559e4ccef996","scrolled":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"pOpVinas_6oo"},"source":["# some dependencies are Na, fill those with the square root of the square\n","train['dependency'] = np.sqrt(train['SQBdependency'])\n","test['dependency'] = np.sqrt(test['SQBdependency'])\n","\n","# fill \"no\"s for education with 0s\n","train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n","train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n","test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n","test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n","\n","# if education is \"yes\" and person is head of household, fill with escolari\n","train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n","train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n","\n","test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n","test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n","\n","# this field is supposed to be interaction between gender and escolari, but it isn't clear what \"yes\" means, let's fill it with 4\n","train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n","train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n","\n","test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n","test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n","\n","# convert to int for our models\n","train['edjefe'] = train['edjefe'].astype(\"int\")\n","train['edjefa'] = train['edjefa'].astype(\"int\")\n","test['edjefe'] = test['edjefe'].astype(\"int\")\n","test['edjefa'] = test['edjefa'].astype(\"int\")\n","\n","# create feature with max education of either head of household\n","train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n","test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n","\n","# fill some nas\n","train['v2a1']=train['v2a1'].fillna(0)\n","test['v2a1']=test['v2a1'].fillna(0)\n","\n","test['v18q1']=test['v18q1'].fillna(0)\n","train['v18q1']=train['v18q1'].fillna(0)\n","\n","train['rez_esc']=train['rez_esc'].fillna(0)\n","test['rez_esc']=test['rez_esc'].fillna(0)\n","\n","train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n","train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n","\n","test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n","test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n","\n","# fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet, \n","# if there is no water we'll assume they do not\n","train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n","train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n","\n","test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n","test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"b564a6552f0521581af1ee38d6040ef7ae5d2fb5","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"TQJpPih5_6oo"},"source":["def train_test_apply_func(train_, test_, func_):\n","    test_['Target'] = 0\n","    xx = pd.concat([train_, test_])\n","\n","    xx_func = func_(xx)\n","    train_ = xx_func.iloc[:train_.shape[0], :]\n","    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n","\n","    del xx, xx_func\n","    return train_, test_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"2695108e103c9c61088fc4c100d01bc8c0f4138c","collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"Jf8mcG-K_6op","executionInfo":{"status":"ok","timestamp":1632293412200,"user_tz":-540,"elapsed":811,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"03a47d95-ec0d-49ab-f754-98ddc7eb7ba6"},"source":["# convert the one hot fields into label encoded\n","train, test = train_test_apply_func(train, test, convert_OHE2LE)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The OHE in techo is incomplete. A new column will be added before label encoding\n","The OHE in instlevel is incomplete. A new column will be added before label encoding\n","The OHE in manual_elec is incomplete. A new column will be added before label encoding\n"]}]},{"cell_type":"markdown","metadata":{"_uuid":"e90e68abd266c9db808dbf336308cef7175886bd","id":"Yc8JPazl_6op"},"source":["# Geo aggregates"]},{"cell_type":"code","metadata":{"_uuid":"0ffc288b3829ffdb1dabf8f2e7fe518f2f520480","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"8ZufQXVA_6op"},"source":["cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n","              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n","              'pared_LE']\n","cols_nums = ['age', 'meaneduc', 'dependency', \n","             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n","             'bedrooms', 'overcrowding']\n","\n","def convert_geo2aggs(df_):\n","    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n","                        pd.get_dummies(df_[cols_2_ohe], \n","                                       columns=cols_2_ohe)],axis=1)#더미 변수로 변환해준다\n","\n","    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n","    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n","    \n","    del tmp_df\n","    return df_.join(geo_agg, how='left', on='lugar_LE')\n","\n","# add some aggregates by geography\n","train, test = train_test_apply_func(train, test, convert_geo2aggs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"bc96d527090d9b0723049c5d763c97be5145b8c7","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"TtFS4fu6_6op"},"source":["# add the number of people over 18 in each household\n","train['num_over_18'] = 0\n","train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n","train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n","train['num_over_18'] = train['num_over_18'].fillna(0)\n","\n","test['num_over_18'] = 0\n","test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n","test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n","test['num_over_18'] = test['num_over_18'].fillna(0)\n","\n","# add some extra features, these were taken from another kernel\n","def extract_features(df):\n","    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n","    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n","    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n","    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n","    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n","    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n","    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n","    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n","    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n","    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n","    # some households have no one over 18, use the total rent for those\n","    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n","    \n","extract_features(train)    \n","extract_features(test)   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"a6ed97e10f021b42a19c7228dfc1a52ce9de2c60","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"KkoMxDup_6op"},"source":["# drop duplicated columns\n","needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n","                 'mobilephone', 'female', ]\n","\n","instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n","\n","needless_cols.extend(instlevel_cols)\n","\n","train = train.drop(needless_cols, axis=1)\n","test = test.drop(needless_cols, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"c6e1ccce811e7a1d76282fcb8a13edf92672f834","id":"c60Z6eyX_6oq"},"source":["## Split the data\n","\n","We split the data by household to avoid leakage, since rows belonging to the same household usually have the same target. Since we filter the data to only include heads of household this isn't technically necessary, but it provides an easy way to use the entire training data set if we want to do that.\n","\n","Note that after splitting the data we overwrite the train data with the entire data set so we can train on all of the data. The split_data function does the same thing without overwriting the data, and is used within the training loop to (hopefully) approximate a K-Fold split. "]},{"cell_type":"code","metadata":{"_uuid":"da37e677a32477006e8468b954e23d595c82eced","id":"B2DThafI_6oq"},"source":["def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n","    # uncomment for extra randomness\n","#     np.random.seed(seed=seed)\n","    \n","    train2 = train.copy()\n","    \n","    # pick some random households to use for the test data\n","    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n","    \n","    # select households which are in the random selection\n","    cv_idx = np.isin(households, cv_hhs)\n","    X_test = train2[cv_idx]\n","    y_test = y[cv_idx]\n","\n","    X_train = train2[~cv_idx] #cv_idx 빼고 전부\n","    y_train = y[~cv_idx]\n","    \n","    if sample_weight is not None:\n","        y_train_weights = sample_weight[~cv_idx]\n","        return X_train, y_train, X_test, y_test, y_train_weights\n","    \n","    return X_train, y_train, X_test, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"e14a9619ca6516b225b55bf65d6a9e423d6b5fc7","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"QQ_NU-zw_6oq"},"source":["X = train.query('parentesco1==1') # query : 조건식을 만족하는 행을 추출한다\n","# X = train.copy()\n","\n","# pull out and drop the target variable\n","y = X['Target'] - 1\n","X = X.drop(['Target'], axis=1)\n","\n","np.random.seed(seed=None)\n","\n","train2 = X.copy()\n","\n","train_hhs = train2.idhogar\n","\n","households = train2.idhogar.unique()\n","cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False)\n","\n","cv_idx = np.isin(train2.idhogar, cv_hhs)\n","\n","X_test = train2[cv_idx]\n","y_test = y[cv_idx]\n","\n","X_train = train2[~cv_idx]\n","y_train = y[~cv_idx]\n","\n","# train on entire dataset\n","X_train = train2\n","y_train = y\n","\n","train_households = X_train.idhogar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"871cd5b46da553f6062cef605acc761cddc2a8c0","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"oVejnavL_6oq"},"source":["# figure out the class weights for training with unbalanced classes\n","y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"d9bcfbfcfc0b5b7e3aaeb2b0c8495bd92fdf51a3","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"IQ1JCkwq_6oq"},"source":["# drop some features which aren't used by the LGBM or have very low importance\n","extra_drop_features = [\n"," 'agg18_estadocivil1_MEAN',\n"," 'agg18_estadocivil6_COUNT',\n"," 'agg18_estadocivil7_COUNT',\n"," 'agg18_parentesco10_COUNT',\n"," 'agg18_parentesco11_COUNT',\n"," 'agg18_parentesco12_COUNT',\n"," 'agg18_parentesco1_COUNT',\n"," 'agg18_parentesco2_COUNT',\n"," 'agg18_parentesco3_COUNT',\n"," 'agg18_parentesco4_COUNT',\n"," 'agg18_parentesco5_COUNT',\n"," 'agg18_parentesco6_COUNT',\n"," 'agg18_parentesco7_COUNT',\n"," 'agg18_parentesco8_COUNT',\n"," 'agg18_parentesco9_COUNT',\n"," 'geo_elimbasu_LE_4',\n"," 'geo_energcocinar_LE_1',\n"," 'geo_energcocinar_LE_2',\n"," 'geo_epared_LE_0',\n"," 'geo_hogar_mayor',\n"," 'geo_manual_elec_LE_2',\n"," 'geo_pared_LE_3',\n"," 'geo_pared_LE_4',\n"," 'geo_pared_LE_5',\n"," 'geo_pared_LE_6',\n"," 'num_over_18',\n"," 'parentesco_LE',\n"," 'rez_esc']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"67a0f3b7d019517ed9a40b41df318284a33a5ce1","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"P4zxapVX_6oq"},"source":["xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"2dc384aeb44db2454978df78fdbb84b2b1ff3ced","id":"0YxzeChj_6oq"},"source":["# Fit a voting classifier\n","Define a derived VotingClassifier class to be able to pass `fit_params` for early stopping. Vote based on LGBM models with early stopping based on macro F1 and decaying learning rate.\n","\n","The parameters are optimised with a random search in this kernel: https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro"]},{"cell_type":"code","metadata":{"_uuid":"629cf88dd0596a98d58487495e5b096636e2586a","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"nmL-sQ7h_6oq"},"source":["# 4\n","opt_parameters = {'max_depth':35, 'eta':0.1, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40 }\n","# 5\n","opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n","# 6\n","# opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n","# # 7\n","# opt_parameters = {'max_depth':35, 'eta':0.12, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 3.25, 'colsample_bylevel': 0.95, 'subsample': 0.88, 'colsample_bytree': 0.88, 'reg_lambda': 0.35 }\n","\n","def evaluate_macroF1_lgb(predictions, truth):  \n","    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n","    pred_labels = predictions.argmax(axis=1)\n","    truth = truth.get_label()\n","    f1 = f1_score(truth, pred_labels, average='macro')\n","    return ('macroF1', 1-f1) \n","\n","fit_params={\"early_stopping_rounds\":500,\n","            \"eval_metric\" : evaluate_macroF1_lgb, \n","            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n","            'verbose': False,\n","           }\n","\n","def learning_rate_power_0997(current_iter):\n","    base_learning_rate = 0.1\n","    min_learning_rate = 0.02\n","    lr = base_learning_rate  * np.power(.995, current_iter)\n","    return max(lr, min_learning_rate)\n","\n","fit_params['verbose'] = 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_uuid":"101a2fd45bbbe6c7fb351513803550d4edeef2b3","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"h-pJIRzD_6oq"},"source":["np.random.seed(100)\n","\n","def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n","    estimator = clone(estimator1)\n","    \n","    # randomly split the data so we have a test set for early stopping\n","    if sample_weight is not None:\n","        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n","    else:\n","        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n","        \n","    # update the fit params with our new split\n","    fit_params[\"eval_set\"] = [(X_test,y_test)]\n","    \n","    # fit the estimator\n","    if sample_weight is not None:\n","        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n","            estimator.fit(X_train, y_train)\n","        else:\n","            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n","    else:\n","        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n","            estimator.fit(X_train, y_train)\n","        else:\n","            _ = estimator.fit(X_train, y_train, **fit_params)\n","    \n","    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n","        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n","        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n","        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n","    else:\n","        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n","        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n","        print(\"Train F1:\", best_train)\n","        print(\"Test F1:\", best_cv)\n","        \n","    # reject some estimators based on their performance on train and test sets\n","    if threshold:\n","        # if the valid score is very high we'll allow a little more leeway with the train scores\n","        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n","            return estimator\n","\n","        # else recurse until we get a better one\n","        else:\n","            print(\"Unacceptable!!! Trying again...\")\n","            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n","    \n","    else:\n","        return estimator\n","    \n","class VotingClassifierLGBM(VotingClassifier):\n","    '''\n","    This implements the fit method of the VotingClassifier propagating fit_params\n","    '''\n","    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n","        \n","        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n","            raise NotImplementedError('Multilabel and multi-output'\n","                                      ' classification is not supported.')\n","\n","        if self.voting not in ('soft', 'hard'):\n","            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n","                             % self.voting)\n","\n","        if self.estimators is None or len(self.estimators) == 0:\n","            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n","                                 ' should be a list of (string, estimator)'\n","                                 ' tuples')\n","\n","        if (self.weights is not None and\n","                len(self.weights) != len(self.estimators)):\n","            raise ValueError('Number of classifiers and weights must be equal'\n","                             '; got %d weights, %d estimators'\n","                             % (len(self.weights), len(self.estimators)))\n","\n","        names, clfs = zip(*self.estimators)\n","        self._validate_names(names)\n","\n","        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n","        if n_isnone == len(self.estimators):\n","            raise ValueError('All estimators are None. At least one is '\n","                             'required to be a classifier!')\n","\n","        self.le_ = LabelEncoder().fit(y)\n","        self.classes_ = self.le_.classes_\n","        self.estimators_ = []\n","\n","        transformed_y = self.le_.transform(y)\n","\n","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n","                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n","                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n","                for clf in clfs if clf is not None)\n","\n","        return self"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"37b909c2aa273651b7bb57c69b939760f14f38f7","scrolled":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"W0nmNJ5E_6oq","outputId":"807fff3c-6fbb-44e7-c644-b880d86fbbe5"},"source":["clfs = []\n","for i in range(15):\n","    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n","    \n","    clfs.append(('xgb{}'.format(i), clf))\n","    \n","vc = VotingClassifierLGBM(clfs, voting='soft')\n","del(clfs)\n","\n","#Train the final model with learning rate decay\n","_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n","\n","clf_final = vc.estimators_[0]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0]\tvalidation_0-merror:0.464646\tvalidation_0-macroF1:0.646101\n","Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n","\n","Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n","[50]\tvalidation_0-merror:0.377104\tvalidation_0-macroF1:0.578445\n","[100]\tvalidation_0-merror:0.367003\tvalidation_0-macroF1:0.570407\n","[150]\tvalidation_0-merror:0.373737\tvalidation_0-macroF1:0.583402\n","[200]\tvalidation_0-merror:0.37037\tvalidation_0-macroF1:0.583715\n","[250]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.583333\n","[299]\tvalidation_0-merror:0.368687\tvalidation_0-macroF1:0.583474\n","Train F1: 0.9153725233385168\n","Test F1: 0.4355385068644192\n","[0]\tvalidation_0-merror:0.469697\tvalidation_0-macroF1:0.640044\n","Multiple eval metrics have been passed: 'validation_0-macroF1' will be used for early stopping.\n","\n","Will train until validation_0-macroF1 hasn't improved in 500 rounds.\n","[50]\tvalidation_0-merror:0.380471\tvalidation_0-macroF1:0.583417\n"]}]},{"cell_type":"code","metadata":{"_uuid":"241d2973497b8eb2e5214f5c8ddb76432576ba35","jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"DIOoDowU_6oq","executionInfo":{"status":"ok","timestamp":1632205945119,"user_tz":-540,"elapsed":454,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"ba2f73f0-8448-4919-a61c-103474125b6b"},"source":["# params 4 - 400 early stop - 15 estimators - l1 used features - weighted\n","global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n","vc.voting = 'soft'\n","global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n","vc.voting = 'hard'\n","global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n","\n","print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n","print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n","print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation score of a single LGBM Classifier: 0.8059\n","Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.9107\n","Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.9345\n"]}]},{"cell_type":"code","metadata":{"_uuid":"527b58d849cc1282909f15596cd5441ef4cac93d","collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"95mIDvY1_6or","executionInfo":{"status":"ok","timestamp":1632206102761,"user_tz":-540,"elapsed":988,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"bb2f1617-c2b6-4eaa-cf04-3f624df108f6"},"source":["# see which features are not used by ANY models\n","useless_features = []\n","drop_features = set()\n","counter = 0\n","for est in vc.estimators_:\n","    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n","    useless_features.append(unused_features)\n","    if counter == 0:\n","        drop_features = set(unused_features)\n","    else:\n","        drop_features = drop_features.intersection(set(unused_features))\n","    counter += 1\n","    \n","drop_features"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'agg18_estadocivil4_COUNT',\n"," 'agg18_estadocivil5_COUNT',\n"," 'geo_energcocinar_LE_0',\n"," 'geo_epared_LE_2',\n"," 'geo_manual_elec_LE_3'}"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"_uuid":"71dae26394955fcf940feea1c9ba1a5bcf134ce9","collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"CUnkqxnP_6or","executionInfo":{"status":"ok","timestamp":1632206109227,"user_tz":-540,"elapsed":522,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"ef91d328-6192-4546-e70d-c46d6816e3b2"},"source":["ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature ranking:\n","1. feature 59 (0.020874) - agg18_escolari_MAX\n","2. feature 42 (0.019113) - fe_children_fraction\n","3. feature 37 (0.017560) - SQBedjefe\n","4. feature 126 (0.016810) - geo_sanitario_LE_3\n","5. feature 74 (0.015333) - agg18_parentesco2_MEAN\n","6. feature 46 (0.014168) - fe_human_bed_density\n","7. feature 22 (0.013392) - dependency\n","8. feature 60 (0.013138) - agg18_escolari_MEAN\n","9. feature 40 (0.012836) - SQBdependency\n","10. feature 135 (0.012545) - geo_pared_LE_7\n","11. feature 112 (0.012330) - geo_etecho_LE_1\n","12. feature 107 (0.011216) - geo_overcrowding\n","13. feature 124 (0.010565) - geo_sanitario_LE_1\n","14. feature 116 (0.010347) - geo_elimbasu_LE_0\n","15. feature 19 (0.010287) - hogar_adul\n","16. feature 34 (0.010142) - SQBescolari\n","17. feature 105 (0.009962) - geo_hogar_total\n","18. feature 23 (0.009942) - edjefe\n","19. feature 94 (0.009886) - etecho_LE\n","20. feature 17 (0.009594) - male\n","21. feature 39 (0.009528) - SQBovercrowding\n","22. feature 41 (0.009366) - SQBmeaned\n","23. feature 44 (0.009305) - fe_all_man_fraction\n","24. feature 96 (0.009216) - estadocivil_LE\n","25. feature 6 (0.009131) - r4h1\n","26. feature 97 (0.009013) - lugar_LE\n","27. feature 65 (0.008972) - agg18_estadocivil3_MEAN\n","28. feature 15 (0.008932) - cielorazo\n","29. feature 16 (0.008922) - dis\n","30. feature 120 (0.008912) - geo_elimbasu_LE_5\n","31. feature 93 (0.008863) - epared_LE\n","32. feature 106 (0.008822) - geo_bedrooms\n","33. feature 104 (0.008821) - geo_hogar_adul\n","34. feature 95 (0.008810) - eviv_LE\n","35. feature 98 (0.008801) - tipovivi_LE\n","36. feature 55 (0.008797) - agg18_age_MIN\n","37. feature 12 (0.008775) - r4t1\n","38. feature 92 (0.008711) - elimbasu_LE\n","39. feature 131 (0.008691) - geo_manual_elec_LE_4\n","40. feature 49 (0.008591) - fe_mobile_density\n","41. feature 35 (0.008552) - SQBage\n","42. feature 111 (0.008547) - geo_etecho_LE_0\n","43. feature 27 (0.008483) - overcrowding\n","44. feature 125 (0.008438) - geo_sanitario_LE_2\n","45. feature 14 (0.008432) - escolari\n","46. feature 5 (0.008364) - v18q1\n","47. feature 33 (0.008358) - age\n","48. feature 13 (0.008337) - r4t2\n","49. feature 87 (0.008315) - piso_LE\n","50. feature 86 (0.008305) - pared_LE\n","51. feature 117 (0.008305) - geo_elimbasu_LE_1\n","52. feature 25 (0.008275) - meaneduc\n","53. feature 123 (0.008270) - geo_sanitario_LE_0\n","54. feature 51 (0.008260) - fe_mobile_adult_density\n","55. feature 69 (0.008140) - agg18_estadocivil5_MEAN\n","56. feature 71 (0.008139) - agg18_estadocivil6_MEAN\n","57. feature 72 (0.008112) - agg18_estadocivil7_MEAN\n","58. feature 45 (0.008103) - fe_human_density\n","59. feature 30 (0.008054) - qmobilephone\n","60. feature 58 (0.008048) - agg18_escolari_MIN\n","61. feature 43 (0.008048) - fe_working_man_fraction\n","62. feature 102 (0.007966) - geo_dependency\n","63. feature 0 (0.007906) - v2a1\n","64. feature 128 (0.007870) - geo_manual_elec_LE_0\n","65. feature 10 (0.007865) - r4m2\n","66. feature 32 (0.007738) - area2\n","67. feature 7 (0.007579) - r4h2\n","68. feature 21 (0.007507) - hogar_total\n","69. feature 109 (0.007466) - geo_eviv_LE_1\n","70. feature 47 (0.007323) - fe_rent_per_person\n","71. feature 142 (0.007268) - hhsize_to_rooms\n","72. feature 63 (0.007225) - agg18_estadocivil2_MEAN\n","73. feature 136 (0.007217) - bedrooms_to_rooms\n","74. feature 1 (0.007192) - hacdor\n","75. feature 138 (0.007158) - tamhog_to_rooms\n","76. feature 50 (0.007148) - fe_tablet_density\n","77. feature 2 (0.007079) - rooms\n","78. feature 29 (0.007009) - television\n","79. feature 91 (0.007006) - energcocinar_LE\n","80. feature 99 (0.007003) - manual_elec_LE\n","81. feature 57 (0.006932) - agg18_age_MEAN\n","82. feature 62 (0.006899) - agg18_estadocivil1_COUNT\n","83. feature 90 (0.006889) - sanitario_LE\n","84. feature 100 (0.006777) - geo_age\n","85. feature 31 (0.006771) - area1\n","86. feature 61 (0.006766) - agg18_dis_MEAN\n","87. feature 24 (0.006738) - edjefa\n","88. feature 8 (0.006734) - r4h3\n","89. feature 52 (0.006694) - fe_tablet_adult_density\n","90. feature 143 (0.006689) - rent_to_hhsize\n","91. feature 11 (0.006668) - r4m3\n","92. feature 122 (0.006643) - geo_energcocinar_LE_3\n","93. feature 48 (0.006560) - fe_rent_per_room\n","94. feature 64 (0.006504) - agg18_estadocivil2_COUNT\n","95. feature 56 (0.006441) - agg18_age_MAX\n","96. feature 20 (0.006432) - hogar_mayor\n","97. feature 26 (0.006391) - bedrooms\n","98. feature 9 (0.006350) - r4m1\n","99. feature 75 (0.006242) - agg18_parentesco3_MEAN\n","100. feature 4 (0.006161) - refrig\n","101. feature 18 (0.005876) - hogar_nin\n","102. feature 101 (0.005755) - geo_meaneduc\n","103. feature 38 (0.005725) - SQBhogar_nin\n","104. feature 103 (0.005689) - geo_hogar_nin\n","105. feature 28 (0.005391) - computer\n","106. feature 140 (0.005380) - r4t3_to_rooms\n","107. feature 88 (0.005258) - techo_LE\n","108. feature 144 (0.005221) - rent_to_over_18\n","109. feature 67 (0.005210) - agg18_estadocivil4_MEAN\n","110. feature 110 (0.005132) - geo_eviv_LE_2\n","111. feature 129 (0.005069) - geo_manual_elec_LE_1\n","112. feature 53 (0.005035) - fe_people_not_living\n","113. feature 79 (0.004939) - agg18_parentesco7_MEAN\n","114. feature 77 (0.004880) - agg18_parentesco5_MEAN\n","115. feature 137 (0.004457) - rent_to_rooms\n","116. feature 114 (0.004299) - geo_epared_LE_1\n","117. feature 81 (0.004282) - agg18_parentesco9_MEAN\n","118. feature 141 (0.004248) - v2a1_to_r4t3\n","119. feature 83 (0.004106) - agg18_parentesco11_MEAN\n","120. feature 133 (0.003824) - geo_pared_LE_1\n","121. feature 3 (0.003191) - hacapo\n","122. feature 78 (0.002986) - agg18_parentesco6_MEAN\n","123. feature 84 (0.002735) - agg18_parentesco12_MEAN\n","124. feature 89 (0.002506) - abastagua_LE\n","125. feature 139 (0.001633) - r4t3_to_tamhog\n","126. feature 76 (0.001460) - agg18_parentesco4_MEAN\n","127. feature 80 (0.000000) - agg18_parentesco8_MEAN\n","128. feature 132 (0.000000) - geo_pared_LE_0\n","129. feature 119 (0.000000) - geo_elimbasu_LE_3\n","130. feature 36 (0.000000) - SQBhogar_total\n","131. feature 82 (0.000000) - agg18_parentesco10_MEAN\n","132. feature 73 (0.000000) - agg18_parentesco1_MEAN\n","133. feature 118 (0.000000) - geo_elimbasu_LE_2\n","134. feature 130 (0.000000) - geo_manual_elec_LE_3\n","135. feature 70 (0.000000) - agg18_estadocivil5_COUNT\n","136. feature 68 (0.000000) - agg18_estadocivil4_COUNT\n","137. feature 115 (0.000000) - geo_epared_LE_2\n","138. feature 66 (0.000000) - agg18_estadocivil3_COUNT\n","139. feature 85 (0.000000) - edjef\n","140. feature 121 (0.000000) - geo_energcocinar_LE_0\n","141. feature 54 (0.000000) - fe_people_weird_stat\n","142. feature 108 (0.000000) - geo_eviv_LE_0\n","143. feature 134 (0.000000) - geo_pared_LE_2\n","144. feature 113 (0.000000) - geo_etecho_LE_2\n","145. feature 127 (0.000000) - geo_sanitario_LE_4\n"]}]},{"cell_type":"markdown","metadata":{"_uuid":"6e78eb892f60a52240aee2b15beaa9f9dfd59994","id":"qX_6DGQp_6or"},"source":["### Random Forest"]},{"cell_type":"code","metadata":{"_uuid":"8a98ed5c6da8cd601a4ff2842c4440c16dcca8b8","collapsed":true,"jupyter":{"outputs_hidden":true},"id":"no6ior5S_6or"},"source":["et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n","       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n","       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n","       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n","       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n","       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n","       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n","       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n","       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n","       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n","       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n","       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n","       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n","       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n","       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n","       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n","       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n","       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n","       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n","       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n","       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN'] #+ ['parentesco_LE', 'rez_esc']\n","\n","et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n","       'fe_tablet_adult_density', 'fe_tablet_density'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"aa255a424ccbd3839015c82777963dc6b79d8cde","collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"mfPkLm4r_6or","executionInfo":{"status":"ok","timestamp":1632206330209,"user_tz":-540,"elapsed":46298,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"76cf8814-429d-4ae8-eb9a-17c0a20d512b"},"source":["# do the same thing for some extra trees classifiers\n","ets = []    \n","for i in range(10):\n","    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n","    ets.append(('rf{}'.format(i), rf))   \n","\n","vc2 = VotingClassifierLGBM(ets, voting='soft')    \n","_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)    "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train F1: 0.8995785797400715\n","Test F1: 0.41644760797490754\n","Train F1: 0.8974057100172891\n","Test F1: 0.4173294959009245\n","Train F1: 0.8929947876700697\n","Test F1: 0.380791318982666\n","Train F1: 0.8897229006999118\n","Test F1: 0.43258661725401276\n","Train F1: 0.8894013811159895\n","Test F1: 0.46517518214144904\n","Train F1: 0.8963246456113938\n","Test F1: 0.426575122901441\n","Train F1: 0.890822628612706\n","Test F1: 0.41104823629263454\n","Train F1: 0.8958771082511664\n","Test F1: 0.46546114892390233\n","Train F1: 0.9064600301011827\n","Test F1: 0.4286664637370844\n","Train F1: 0.8948837246950454\n","Test F1: 0.41037179469569823\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hrgIT5Rb3mDy","executionInfo":{"status":"ok","timestamp":1632220017896,"user_tz":-540,"elapsed":6461,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"f3ac41fc-2c29-4edf-fe81-fd120d38c046"},"source":["# w/ threshold, extra drop cols\n","vc2.voting = 'soft'\n","global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n","vc2.voting = 'hard'\n","global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n","\n","print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n","print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: 0.9107\n","Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: 0.9345\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQ419Nb-5REv","executionInfo":{"status":"ok","timestamp":1632220331799,"user_tz":-540,"elapsed":2636,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"9deb216a-9df7-458f-a26c-afa00277c82d"},"source":["# see which features are not used by ANY models\n","\n","useless_features = []\n","drop_features= set()\n","counter = 0\n","for est in vc2.estimators_:\n","  ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1),display_results=False)\n","  useless_features.append(unused_features)\n","  if counter == 0:\n","    drop_features = set(unused_features)\n","\n","  else:\n","    drop_features = drop_features.intersection(set(unused_features))\n","  counter += 1\n","\n","drop_features"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'parentesco_LE', 'rez_esc'}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"sTLUh5ms5Q3G"},"source":["def combine_voters(data, weights=[0.5, 0.5]):\n","  # do soft voting with both classifiers\n","  vc.voting='soft'\n","  vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n","  vc2.voting = 'soft'\n","  vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n","\n","  final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n","  predictions = np.argmax(final_vote, axis=1)\n","\n","  return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwNzHBDY7ksC","executionInfo":{"status":"ok","timestamp":1632220721015,"user_tz":-540,"elapsed":3980,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"d52cc5b2-e0f8-4b51-94b1-0d78f09b32f9"},"source":["combo_preds = combine_voters(X_test, weights=[0.5,0.5])\n","global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n","global_combo_score_soft"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9063856401043345"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4-0s4SEZ78LM","executionInfo":{"status":"ok","timestamp":1632220784935,"user_tz":-540,"elapsed":3215,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"aee08de2-2138-4a19-957b-a72b9340cf38"},"source":["combo_preds = combine_voters(X_test, weights=[0.4,0.6])\n","global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n","global_combo_score_soft"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9044267606802022"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QkT-SvtZ779V","executionInfo":{"status":"ok","timestamp":1632220800558,"user_tz":-540,"elapsed":3321,"user":{"displayName":"량량양","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01506279447016669318"}},"outputId":"213f5f84-bd77-41f6-c261-249d66a7e6c9"},"source":["combo_preds = combine_voters(X_test, weights=[0.6,0.4])\n","global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n","global_combo_score_soft"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9063856401043345"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"A0kSZ1kq8Pb3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OhtmxCe8PUk"},"source":[""],"execution_count":null,"outputs":[]}]}